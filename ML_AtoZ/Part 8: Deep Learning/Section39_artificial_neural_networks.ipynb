{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning A-Z™\n",
    "\n",
    "© Kirill Eremenko, Hadelin de Ponteves, SuperDataScience Team |\n",
    "[Super Data Science](http://www.superdatascience.com)\n",
    "\n",
    "Part 8: Deep Learning | Section 36: Artificial Neural Networks (ANN)\n",
    "\n",
    "Created on Tue Apr  20, 2019\n",
    "@author: yinka_ola\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---\n",
    "\n",
    "## Deep Learning:\n",
    "## Deep Learning is the most exciting and powerful branch of Machine Learning. \n",
    "## Deep Learning models can be used for a variety of complex tasks:\n",
    "\n",
    "## Artificial Neural Networks for Regression and Classification\n",
    "## Convolutional Neural Networks for Computer Vision\n",
    "## Recurrent Neural Networks for Time Series Analysis\n",
    "## Self Organizing Maps for Feature Extraction\n",
    "## Deep Boltzmann Machines for Recommendation Systems\n",
    "## Auto Encoders for Recommendation Systems\n",
    "\n",
    "## In this part, you will understand and learn how to implement the following \n",
    "## Deep Learning models: Artificial Neural Networks for a Business Problem\n",
    "\n",
    "## ---\n",
    "\n",
    "## Deep Learning History:\n",
    "## Invented in the 70s, caugth on in the 80s\n",
    "## died off over the next decade => computational processing power not available\n",
    "## huge jump in technology progress = exponential growth\n",
    "## future: DNA data storage\n",
    "## 1kg DNA to store all of the worlds data\n",
    "\n",
    "## So what is deep learning?\n",
    "## Geoffery Hinton: Godfather of Deep learning: currently at Google\n",
    "## goal: mimic how the human brain operate and recreate it\n",
    "## human brain: one of the most powerful machine of learning\n",
    "## approx. 100 billion neuron in the human brain\n",
    "## create an artificial structure = artificail neural net\n",
    "## input layer + hidden layer + output layer\n",
    "## multiple hidden layer and connect each together \n",
    "\n",
    "## ---\n",
    "\n",
    "## Artificial Neural Networks (ANN):\n",
    "## the neuron\n",
    "## the activation function\n",
    "## how do neural networks work? (example)\n",
    "## how neural networks learn\n",
    "## Gradient descent\n",
    "## Stochastic gradient descent\n",
    "## Backpropagation\n",
    "\n",
    "## ---\n",
    "## The neuron:\n",
    "## goal of ANN: recreate the neuron\n",
    "## Neuron, dendrites, axon\n",
    "## a sole neuron  is not strong // but much more effective in groups\n",
    "## neurons transmit signals via the axon // synopses\n",
    "\n",
    "## input value => neuron => output value\n",
    "## output value can be either continuous  or categorical\n",
    "## same input row = same row into output\n",
    "## synopsis get assigned weights = ANN learns/trains by adjusting weights\n",
    "## what happens inside the neuron?\n",
    "\n",
    "## The activation function:\n",
    "## The threshold function (binary)\n",
    "## the sigmoid funtion = smoother, no kinks like the threshold function (binary)\n",
    "## The rectifier function\n",
    "## hyperbolic tangent (tanh)\n",
    "\n",
    "## How neural network works:\n",
    "## case study: property valuation\n",
    "## input layer: area,bedrooms, distance to city, age, etc.\n",
    "## output layer: price of house (no hidden layer)\n",
    "## add hidden layers to see how it grants extra power to computation\n",
    "## hidden layer uses combination of input layer\n",
    "## combination of layers creates powerful prediction computation\n",
    "\n",
    "## how neural networks learn?\n",
    "## ex. image recognition\n",
    "## 1. hard coding: a lot of rules to be specified (i.e.)\n",
    "## 2. create a facility so NN can understand and learn on its own\n",
    "## goal of 2: avoid inputting rules\n",
    "## here you program an architecture and point it into the folder to learn\n",
    "## cost function\n",
    "## in order for a  NN to learn: back propagation of weight adjustment occurs\n",
    "\n",
    "## Gradient descent:\n",
    "## the curse of dimensionality\n",
    "## sunway taihulight: world fastest super computer 93 PFLOPSec\n",
    "## 10^50 years (longer than age of universe) to compute 10^75combinations = No\n",
    "## solution: gradient descent\n",
    "## reduces computational time to minutes\n",
    "## roll the ball down a parabola = to find the best weight that minize cost fcn\n",
    "## you are descending to the minimum cost function\n",
    "## requires cost function to be convex\n",
    "\n",
    "## Stochastic Gradient Descent:\n",
    "## what if cost function is not convex? (multi-dmimensional space)\n",
    "## goal: we want to global (not local) minimum\n",
    "## solution: stochastic gradient descent\n",
    "## here adjust the weights after evaluating each row/iteration\n",
    "## rows are picked at random\n",
    "## it helps avoid descovering local minimum from global minimum \n",
    "## batch gradient descent method: a deterministic algorithm (same result)\n",
    "\n",
    "## Backpropagation\n",
    "## It adjust all the weights at the same time\n",
    "\n",
    "## Steps to training ANN via Stochastic gradient descent\n",
    "## 1. randomly initize the weights to small numbers close to 0 (not 0)\n",
    "## 2. input the first observation of dataset in input layer, 1 feature per input node\n",
    "## 3. forward propagation: from left to right. neurons are activated such that\n",
    "## impact of each neuron's activation is limited by weights. propagate \n",
    "## activations until you get predicted result y\n",
    "## 4. compare predicted result to the actual result. measure generated error\n",
    "## 5. back propagation: Right to Left, error is back propagated. Update weights\n",
    "## according to contrbution to error. learning rate decides by how much weight is adjusted\n",
    "## 6. repreat 1 to 5 + update weights after each obs (reinforcement learning) Or:\n",
    "## repeat 1 to 5  but update weights only after a batch of obs (Batch Learning).\n",
    "## 7. when whole training set passed theough ANN => Epoch. Redo more epochs\n",
    "\n",
    "## ---\n",
    "\n",
    "#Data Scenario: \n",
    "## a bank snapshot of 10k randomly selected customers\n",
    "## bank is experiencing unsually high churn rate\n",
    "## investigate what is causing the churn and recommend a solution\n",
    "## 6 months investigation: did they stay or leave the bank?\n",
    "## determine which of the customers are going to leave? \n",
    "## determine factors which influences the outcome\n",
    "## note: this analysis is transferrable to any industry\n",
    "\n",
    "## Python Libraries and Packages\n",
    "## Keras package integrates TensorFlow\n",
    "## in mac/pc terminal: conda install -c conda-forge keras\n",
    "## Theano: fast numerical computation library\n",
    "## GPU: processor for graphic puposes: much more powerful than CPU\n",
    "## Tensor: numerical computation\n",
    "## Theano + Tensor flow: for research purposes ( a lot of lines of code)\n",
    "## Keras: based on Theano + Tensor flow (few lines of code)\n",
    "## use keras to build deep learning algorithm efficiently\n",
    "\n",
    "## ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd #data\n",
    "import numpy as np #mathematics\n",
    "import os\n",
    "#plotting packages\n",
    "import matplotlib.pyplot as plt #plotting charts\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10,5\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1 - Data Preprocessing\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "x = dataset.iloc[:, 3:13].values #upperbound is excluded in a range\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 6.1900000e+02 ... 1.0000000e+00\n",
      "  1.0000000e+00 1.0134888e+05]\n",
      " [0.0000000e+00 1.0000000e+00 6.0800000e+02 ... 0.0000000e+00\n",
      "  1.0000000e+00 1.1254258e+05]\n",
      " [0.0000000e+00 0.0000000e+00 5.0200000e+02 ... 1.0000000e+00\n",
      "  0.0000000e+00 1.1393157e+05]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 7.0900000e+02 ... 0.0000000e+00\n",
      "  1.0000000e+00 4.2085580e+04]\n",
      " [1.0000000e+00 0.0000000e+00 7.7200000e+02 ... 1.0000000e+00\n",
      "  0.0000000e+00 9.2888520e+04]\n",
      " [0.0000000e+00 0.0000000e+00 7.9200000e+02 ... 1.0000000e+00\n",
      "  0.0000000e+00 3.8190780e+04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data (independent variables)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_x_1 = LabelEncoder()\n",
    "x[:, 1] = labelencoder_x_1.fit_transform(x[:, 1])\n",
    "labelencoder_x_2 = LabelEncoder()\n",
    "x[:, 2] = labelencoder_x_2.fit_transform(x[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1]) #no order to categorical var.\n",
    "x = onehotencoder.fit_transform(x).toarray()\n",
    "x = x[:, 1:]\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "# it is needed, due to high computational settings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 6s 763us/step - loss: 0.4907 - acc: 0.7955\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 3s 405us/step - loss: 0.4284 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 3s 425us/step - loss: 0.4213 - acc: 0.8079\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 3s 410us/step - loss: 0.4148 - acc: 0.8279\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 3s 434us/step - loss: 0.4098 - acc: 0.8326\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 3s 407us/step - loss: 0.4068 - acc: 0.8350\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 4s 515us/step - loss: 0.4046 - acc: 0.8350\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 5s 571us/step - loss: 0.4028 - acc: 0.8340\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 5s 629us/step - loss: 0.4013 - acc: 0.8346\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 3s 427us/step - loss: 0.4005 - acc: 0.8352\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 3s 406us/step - loss: 0.3992 - acc: 0.8346\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 3s 386us/step - loss: 0.3983 - acc: 0.8350\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 3s 387us/step - loss: 0.3983 - acc: 0.8371\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 4s 497us/step - loss: 0.3977 - acc: 0.8356\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 3s 418us/step - loss: 0.3975 - acc: 0.8357\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 3s 390us/step - loss: 0.3971 - acc: 0.8349\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 3s 360us/step - loss: 0.3963 - acc: 0.8356\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 3s 382us/step - loss: 0.3962 - acc: 0.8336\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 3s 401us/step - loss: 0.3955 - acc: 0.8360\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 0.3957 - acc: 0.8359\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 3s 379us/step - loss: 0.3949 - acc: 0.8366\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 3s 391us/step - loss: 0.3944 - acc: 0.8347\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 3s 407us/step - loss: 0.3942 - acc: 0.8360\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 3s 388us/step - loss: 0.3932 - acc: 0.8347\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 3s 430us/step - loss: 0.3930 - acc: 0.8374\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 3s 389us/step - loss: 0.3922 - acc: 0.8379\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 3s 393us/step - loss: 0.3911 - acc: 0.8395\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 3s 407us/step - loss: 0.3903 - acc: 0.8375\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 3s 391us/step - loss: 0.3882 - acc: 0.8392\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 3s 373us/step - loss: 0.3863 - acc: 0.8382\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 3s 383us/step - loss: 0.3844 - acc: 0.8400\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 3s 397us/step - loss: 0.3818 - acc: 0.8395\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 3s 390us/step - loss: 0.3797 - acc: 0.8409\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 3s 373us/step - loss: 0.3771 - acc: 0.8414\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 3s 401us/step - loss: 0.3750 - acc: 0.8416\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 4s 451us/step - loss: 0.3730 - acc: 0.8406\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 3s 422us/step - loss: 0.3715 - acc: 0.8420\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 3s 398us/step - loss: 0.3697 - acc: 0.8435\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 3s 413us/step - loss: 0.3679 - acc: 0.8436\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 0.3667 - acc: 0.8437\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 0.3647 - acc: 0.8440\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 3s 429us/step - loss: 0.3637 - acc: 0.8452\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 3s 393us/step - loss: 0.3617 - acc: 0.8459\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 4s 440us/step - loss: 0.3597 - acc: 0.8459\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 4s 499us/step - loss: 0.3573 - acc: 0.8522\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 4s 502us/step - loss: 0.3565 - acc: 0.8506\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 4s 544us/step - loss: 0.3545 - acc: 0.8541\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 4s 473us/step - loss: 0.3527 - acc: 0.8521\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 4s 526us/step - loss: 0.3511 - acc: 0.8560\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 6s 735us/step - loss: 0.3513 - acc: 0.8549\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 5s 635us/step - loss: 0.3505 - acc: 0.8585\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 4s 516us/step - loss: 0.3490 - acc: 0.8571\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 3s 409us/step - loss: 0.3489 - acc: 0.8566\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 4s 457us/step - loss: 0.3473 - acc: 0.8560\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 4s 462us/step - loss: 0.3476 - acc: 0.8557\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 3s 398us/step - loss: 0.3476 - acc: 0.8580\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 3s 427us/step - loss: 0.3469 - acc: 0.8569\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 3s 392us/step - loss: 0.3472 - acc: 0.8570\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 3s 374us/step - loss: 0.3461 - acc: 0.8587\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 3s 366us/step - loss: 0.3451 - acc: 0.8576\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 3s 364us/step - loss: 0.3473 - acc: 0.8579\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 3s 377us/step - loss: 0.3451 - acc: 0.8581\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 3s 359us/step - loss: 0.3466 - acc: 0.8582\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 3s 359us/step - loss: 0.3451 - acc: 0.8602\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 3s 425us/step - loss: 0.3443 - acc: 0.8585\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 4s 472us/step - loss: 0.3434 - acc: 0.8590\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 4s 443us/step - loss: 0.3440 - acc: 0.8579\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 3s 420us/step - loss: 0.3454 - acc: 0.8581\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 4s 439us/step - loss: 0.3434 - acc: 0.8596\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 4s 438us/step - loss: 0.3446 - acc: 0.8592\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 3s 359us/step - loss: 0.3449 - acc: 0.8582\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 3s 351us/step - loss: 0.3439 - acc: 0.8606\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 3s 360us/step - loss: 0.3438 - acc: 0.8565\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 3s 390us/step - loss: 0.3443 - acc: 0.8579\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 3s 374us/step - loss: 0.3445 - acc: 0.8561\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 3s 368us/step - loss: 0.3436 - acc: 0.8581\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 3s 354us/step - loss: 0.3443 - acc: 0.8597\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 3s 381us/step - loss: 0.3446 - acc: 0.8589\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 3s 385us/step - loss: 0.3442 - acc: 0.8595\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 3s 403us/step - loss: 0.3434 - acc: 0.8591\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 3s 404us/step - loss: 0.3435 - acc: 0.8579\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 3s 387us/step - loss: 0.3436 - acc: 0.8582\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 4s 501us/step - loss: 0.3437 - acc: 0.8591\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 3s 365us/step - loss: 0.3434 - acc: 0.8576\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 3s 371us/step - loss: 0.3428 - acc: 0.8579\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 3s 385us/step - loss: 0.3429 - acc: 0.8576\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 3s 423us/step - loss: 0.3436 - acc: 0.8579\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 4s 444us/step - loss: 0.3427 - acc: 0.8599\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 4s 455us/step - loss: 0.3436 - acc: 0.8594\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 3s 437us/step - loss: 0.3428 - acc: 0.8602\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 3s 414us/step - loss: 0.3429 - acc: 0.8591\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 3s 392us/step - loss: 0.3434 - acc: 0.8585\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 3s 379us/step - loss: 0.3421 - acc: 0.8587\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 4s 446us/step - loss: 0.3428 - acc: 0.8566\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 3s 402us/step - loss: 0.3423 - acc: 0.8596\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 3s 381us/step - loss: 0.3428 - acc: 0.8586\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 3s 418us/step - loss: 0.3431 - acc: 0.8584\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 3s 398us/step - loss: 0.3425 - acc: 0.8602\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 4s 520us/step - loss: 0.3423 - acc: 0.8586\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 3s 391us/step - loss: 0.3426 - acc: 0.8595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3b022ef0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2 - Now let's make the ANN!\n",
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense #module to create layer in ANN\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "## we have 11 independent variables\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "## not useful for our dataset, but good to know how to do it.\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "## if dependent variable has > 2 categories = use soft max \n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(x_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Observation: our accuracy converges to 85.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-766d005fae89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## this is the probability that the 2000 test customer will leave the bank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## we need a threshold to determine if category is 0 or 1 => 50% threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "## Part 3 - Making the predictions and evaluating the model \n",
    "# Predicting the Test set results\n",
    "## this is the probability that the 2000 test customer will leave the bank\n",
    "## we need a threshold to determine if category is 0 or 1 => 50% threshold\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bank can use this to make prediction on training set if accuracy is close to \n",
    "## 83.5% then can rank them and target customers most likely to leave and create \n",
    "## to keep them from leaving measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm the accuracy\n",
    "## on new observation, we get an accuracy of 86% => same accuracy as training\n",
    "## bank can use this to predict to predict if client will stay \n",
    "(1550 +175)/2000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
