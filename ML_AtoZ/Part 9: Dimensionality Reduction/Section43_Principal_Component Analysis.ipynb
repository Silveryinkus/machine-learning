{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning A-Z™\n",
    "\n",
    "© Kirill Eremenko, Hadelin de Ponteves, SuperDataScience Team |\n",
    "[Super Data Science](http://www.superdatascience.com)\n",
    "\n",
    "Part 9: Dimentionality Reduction | Section 43: Principal Component Analysis (PCA)\n",
    "\n",
    "Created on Apr  22, 2019\n",
    "@author: yinka_ola\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---\n",
    "\n",
    "## Remember in Part 3 - Classification, we worked with datasets composed \n",
    "## of only two independent variables. We did for two reasons:\n",
    "\n",
    "## 1. Because we needed two dimensions to visualize better how Machine Learning \n",
    "## models worked (by plotting the prediction regions and the prediction boundary \n",
    "## for each model).\n",
    "## 2. Because whatever is the original number of our independent variables, \n",
    "## we can often end up with two independent variables by applying an appropriate \n",
    "## Dimensionality Reduction technique.\n",
    "\n",
    "## There are two types of Dimensionality Reduction techniques:\n",
    "## 1.Feature Selection\n",
    "## 2.Feature Extraction\n",
    "\n",
    "## Feature Selection techniques are Backward Elimination, Forward Selection, \n",
    "## Bidirectional Elimination, Score Comparison and more. \n",
    "## We covered these techniques in Part 2 - Regression.\n",
    "\n",
    "## In this part we will cover the following Feature Extraction techniques:\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "## ---\n",
    "\n",
    "## Principal Component Analysis (PCA):\n",
    "## Most used unsupervised algorithm + most populat dimentionality reduction \n",
    "\n",
    "## Application:\n",
    "## Noise Filtering\n",
    "## Visualization\n",
    "## Feature Extraction\n",
    "## Stock Market Predictions\n",
    "## Gene data analysis\n",
    "\n",
    "## goal of PCA:\n",
    "## 1. identify the patterns in data\n",
    "## 2. detect correlation b/t variables\n",
    "## reduce dimension of a d-dimensional dataset by projecting it onto a k\n",
    "## dimensional subspace (where k< d)\n",
    "## learn about relationship b/t x and Y values\n",
    "## find list of principal axes\n",
    "\n",
    "## PCA in 2D vs 3D: http://setosa.io/ev/principal-component-analysis/\n",
    "\n",
    "## summary:\n",
    "## from m independent variable of dataset, PCA extracts p =< m new independent\n",
    "## that explain the most variance in the dataset regardless of the dependent variable\n",
    "## b/c dependent variable is not considered, PCA = unsupervised model\n",
    "\n",
    "## ---\n",
    "\n",
    "#Data Scenario: \n",
    "## winery collected information on \n",
    "## using clustering technique: created 3 customer segments\n",
    "## 3 types of wine for each customer segment\n",
    "## what business owner can do can take information + customer segment and create a\n",
    "## classification model\n",
    "## wine owner can predict which new wines to be recommended to each customer segment\n",
    "## for a visual look: it cannot be done with 12 independent variables\n",
    "## we needdimenality reduction techniques to extract 2-3 independent variables with most impact\n",
    "## use PCA to pick 2-3 top influencial \n",
    "## the extrated features are called principal components\n",
    "\n",
    "## ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd #data\n",
    "import numpy as np #mathematics\n",
    "import os\n",
    "#plotting packages\n",
    "import matplotlib.pyplot as plt #plotting charts\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10,10\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Wine.csv')\n",
    "\n",
    "## independent variable\n",
    "x = dataset.iloc[:, 0:13].values\n",
    "\n",
    "## dependent variable\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "## must be applied when using dimensionality reduction\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA\n",
    "from sklearn.decomposition import PCA\n",
    "## this is the number of principal components that explains the most variance\n",
    "pca = PCA(n_components = 2) #create an object of the PCA class\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "explained_variance = pca.explained_variance_ratio_ #an attribute of PCA class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix to evaluate the model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Observation\n",
    "## here confusion matrix with 3 classes (3 x 3)\n",
    "## here the diagonals contains the correct results\n",
    "## 14 correct predictions of customer segment 1\n",
    "## 15 correct predictions of customer segment 2\n",
    "## 6 correct predictions of customer segment 3\n",
    "# we have almost no incorrect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Accuracy check\n",
    "35/36 #97.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Training set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "x_set, y_set = x_train, y_train\n",
    "x1, x2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(x1, x2, classifier.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('darkred', 'green', 'blue'))) # add 1 more colur for 3rd classes\n",
    "plt.xlim(x1.min(), x1.max())\n",
    "plt.ylim(x2.min(), x2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "x_set, y_set = x_test, y_test\n",
    "x1, x2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(x1, x2, classifier.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
    "plt.xlim(x1.min(), x1.max())\n",
    "plt.ylim(x2.min(), x2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
